{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from  xgboost import XGBClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.utils import resample\n",
    "from scipy import stats\n",
    "from sklearn import metrics\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier \n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n",
    "# Introduction\n",
    "### In this notebook I want to study thyroid problems and I want to train and prepare some classifiers that could recognize any kind of thyroid problem in a patient.<br>To do this, I will use six datasets I got from -> https://archive.ics.uci.edu/ml/datasets/Thyroid+Disease:\n",
    "* allhyperTestEDIT and allhyperTrainEDIT present classes'hyperthyroid','T3 toxic','goitre','secondary toxic' and 'negative'\n",
    "* allhypoDATA and allhypoTEST present classes 'hypothyroid','primary hypothyroid','compensated hypothyroid','secondary hypothyroid' and 'negative'\n",
    "* hypothyroid present classes 'hypothyroid' and 'negative'\n",
    "* sick-euthyroid present classes 'sick-euthyroid' and 'negative'\n",
    "* thyroid0387 present classes hyperthyroid conditions (A, B, C, D), hypothyroid conditions (E, F, G, H), binding protein (I, J), general health (K), replacement therapy (L, M, N), discordant results (R) \n",
    "* ann-test and ann-train present classes normal (not hypothyroid), hyperfunction and subnormal functioning\n",
    "\n",
    "### <br> You can find the full documentation on the link above. I want to build a dataset, merging these six above, which present only three classes: hypothyroid, hyperthyroid and negative. Once this work is done, I will go on with the data pre processing and then I will train and test the classifiers.\n",
    "# Part 1: Data Integration\n",
    "### I have to integrate these six different datasets. I will start from the 'all' series because they have the same scheme. \n",
    "allHyperTest = pd.read_csv(\"../input/hypothyroid-multi-dataset/allhyperTestEDIT.CSV\")\n",
    "allHyperTrain = pd.read_csv(\"../input/hypothyroid-multi-dataset/allhyperTrainEDIT.CSV\")\n",
    "allHypoTest = pd.read_csv(\"../input/hypothyroid-multi-dataset/allhypoTEST.csv\")\n",
    "allHypoTrain = pd.read_csv(\"../input/hypothyroid-multi-dataset/allhypoDATA.CSV\")\n",
    "\n",
    "display(allHypoTest.head(10))\n",
    "display(allHypoTrain.dtypes)\n",
    "### ID is an identificator, so I have to check if there are any istances with the same value for this attribute:\n",
    "def handleDuplicated(df):\n",
    "    if df[\"ID\"].duplicated().sum() == 0 :\n",
    "        print(\"There aren't duplicates\")\n",
    "    elif (df[\"ID\"].duplicated().sum()) < len(df) / 100:\n",
    "        df[\"ID\"].drop_duplicates(keep=\"first\", inplace=True)\n",
    "        print(\"duplicates were less than the 1% of all the data, they have been dropped\")\n",
    "    else:\n",
    "        index_duplicated = df[\"ID\"].duplicated().index\n",
    "        print(\"duplicates are more than the 1% of all the data, they have been preserved\")\n",
    "        print(index_duplicated)\n",
    "\n",
    "handleDuplicated(allHyperTest)\n",
    "handleDuplicated(allHyperTrain)\n",
    "handleDuplicated(allHypoTest)\n",
    "handleDuplicated(allHypoTrain)\n",
    " ### Now it's possible to drop the ID attribute because it's useless for the classification:\n",
    "del allHyperTest[\"ID\"]\n",
    "del allHyperTrain[\"ID\"]\n",
    "del allHypoTest[\"ID\"]\n",
    "del allHypoTrain[\"ID\"]\n",
    "### From these four datasets I will obtain all the istances presentig a class that's different from 'negative':\n",
    "def notCorrect_TargetFilter(df,correct_Target,target):\n",
    "    df = df[df.Target.isin(correct_Target)]\n",
    "    df.replace(correct_Target,target,inplace = True)\n",
    "    return df\n",
    "    \n",
    "allHyperTest = notCorrect_TargetFilter(allHyperTest,[\"hyperthyroid\",\"T3_toxic\",\"goitre\",\"secondary_toxic\"],\"hyperthyroid\")\n",
    "allHyperTrain = notCorrect_TargetFilter(allHyperTrain,[\"hyperthyroid\",\"T3_toxic\",\"goitre\",\"secondary_toxic\"],\"hyperthyroid\")\n",
    "allHypoTest = notCorrect_TargetFilter(allHypoTest,[\"hypothyroid\", \"primary_hypothyroid\", \"compensated_hypothyroid\", \"secondary_hypothyroid\"],\"hypothyroid\")\n",
    "allHypoTrain = notCorrect_TargetFilter(allHypoTrain,[\"hypothyroid\", \"primary_hypothyroid\", \"compensated_hypothyroid\", \"secondary_hypothyroid\"],\"hypothyroid\")\n",
    "### Now I will merge the four datasets:\n",
    "allDataset = pd.concat([allHyperTest,allHyperTrain,allHypoTest,allHypoTrain], ignore_index = True)\n",
    "display(allDataset.shape)\n",
    "### That's all for the 'all' series. Let's go on with thyroid0387:\n",
    "thyroid0387 = pd.read_csv(\"../input/hypothyroid-multi-dataset/thyroid0387EDIT.CSV\")\n",
    "display(thyroid0387.head(10))\n",
    "display(thyroid0387.dtypes)\n",
    "### We have the ID attribute here too, so:\n",
    "handleDuplicated(thyroid0387)\n",
    "del thyroid0387[\"ID\"]\n",
    "### This dataset has different interesting classes: A,B,C,D,E,F,G,H. All the others should be considered as 'negative'. I have to be careful because 'F' and 'M' are used in the 'sex' attribute too, so before any sostitution, I have to handle this problem:\n",
    "thyroid0387['sex'] = thyroid0387['sex'].map({'F': 1, 'M': 0})\n",
    "\n",
    "thyroid0387.replace(['A','B','C','D'],\"hyperthyroid\",inplace = True)\n",
    "thyroid0387.replace(['E','F','G','H'],\"hypothyroid\",inplace = True)\n",
    "\n",
    "for value in set(thyroid0387['Target']):\n",
    "    if(value != 'hypothyroid' and value != 'hyperthyroid'):\n",
    "        thyroid0387.replace(value,'negative',inplace=True)\n",
    "### Let's continue with the 'hypothyroid' dataset:\n",
    "hypothyroid = pd.read_csv(\"../input/hypothyroid-multi-dataset/hypothyroid.csv\")\n",
    "display(hypothyroid.shape)\n",
    "display(hypothyroid.head(10))\n",
    "display(hypothyroid.dtypes)\n",
    "### The 'Unnamed' attribute indicate the class of the istance, so I have to rename it. Then I will filter the 'hypothyroid' class istances. For this dataset I don't have 'I131_treatment', 'hypopituitary', 'psych' and 'referral_source' attributes.\n",
    "hypothyroid = hypothyroid.rename(columns={hypothyroid.columns[0]:\"Target\",hypothyroid.columns[1]:\"age\",hypothyroid.columns[2]:\"sex\" })\n",
    "hypothyroid = hypothyroid[hypothyroid.Target.isin(['hypothyroid'])]\n",
    "### For 'sick-euthyroid' I have to filter all the 'negative' istances:\n",
    "sick_euthyroid = pd.read_csv(\"../input/hypothyroid-multi-dataset/sick-euthyroid.CSV\")\n",
    "display(sick_euthyroid.shape)\n",
    "display(sick_euthyroid.head(10))\n",
    "display(sick_euthyroid.dtypes)\n",
    "### For this dataset I don't have 'I131_treatment', 'hypopituitary', 'psych' and 'referral_source' attributes.\n",
    "sick_euthyroid = sick_euthyroid[sick_euthyroid.Target.isin(['negative'])]\n",
    "display(sick_euthyroid.shape)\n",
    "### Now it's time to work on the \"ann\" series:\n",
    "ann_train = pd.read_csv(\"../input/hypothyroid-multi-dataset/ann-train.CSV\")\n",
    "ann_test = pd.read_csv(\"../input/hypothyroid-multi-dataset/ann-test.CSV\")\n",
    "display(ann_test.head(10))\n",
    "display(ann_test.dtypes)\n",
    "### I don't have 'measured' attributes, the 'TBG' and the 'referral_source' attributes. I should create the 'measured' attributes basing on the other columns.\n",
    "target1 = pd.Series(ann_test[ann_test.columns[-1]].values)\n",
    "display(target1.value_counts())\n",
    "target2 = pd.Series(ann_train[ann_train.columns[-1]].values)\n",
    "display(target2.value_counts())\n",
    "### Looking at the distribuition of the values for the 'Target' attribute, we can understand that:\n",
    "* 3 is referring to the 'negative' class\n",
    "* 2 is referring to the 'hypothyroid' class\n",
    "* 1 is referring to the 'hyperthyroid' class\n",
    "\n",
    "### I should analyze the distribuition of the sex attribute in the other datasets to understand how I should treat it in the 'ann' series:\n",
    "print(\"Sex thyroid0387 1=F,0=M:\")\n",
    "sex_series1 = pd.Series(thyroid0387[thyroid0387.columns[1]].values)\n",
    "display(sex_series1.value_counts())\n",
    "print(\"Sick-euthyroid:\")\n",
    "sex_series2 = pd.Series(sick_euthyroid[sick_euthyroid.columns[2]].values)\n",
    "display(sex_series2.value_counts())\n",
    "### So, there are more female than male patients in these datasets. Looking at the \"ann\" series I got:\n",
    "sex1 = pd.Series(ann_test[ann_test.columns[1]].values)\n",
    "display(sex1.value_counts())\n",
    "sex2 = pd.Series(ann_train[ann_train.columns[1]].values)\n",
    "display(sex2.value_counts())\n",
    "### I can assume that '0' refers to female patients and '1' refers to male patients. Another important things to do is to multply for 100 all the continuos and numerical attributes and to add the 'measured' attributes.\n",
    "for column in ann_train.columns:\n",
    "    listOfValues=set(ann_train[column])\n",
    "    print(column,\": \",listOfValues)\n",
    "ann = pd.concat([ann_train,ann_test], ignore_index = True)\n",
    "ann['sex'] = ann['sex'].map({0:'F',1:'M'})\n",
    "ann['Target'] = ann['Target'].map({3:'negative',2:'hypothyroid',1:'hyperthyroid'})\n",
    "\n",
    "continuos_attributes = ['age','TSH','T3','TT4','T4U','FTI']\n",
    "for attribute in continuos_attributes:\n",
    "    ann[attribute] = ann[attribute] * 100\n",
    "\n",
    "def fillNewAttributes(row,attribute):\n",
    "    if row[attribute] > 0:\n",
    "        return 'y'\n",
    "    else:\n",
    "        return 'n'\n",
    "\n",
    "ann['TSH_measured'] = ann.apply(lambda row: fillNewAttributes(row,'TSH'), axis=1)\n",
    "ann['T3_measured'] = ann.apply(lambda row: fillNewAttributes(row,'T3'), axis=1)\n",
    "ann['TT4_measured'] = ann.apply(lambda row: fillNewAttributes(row,'TT4'), axis=1)\n",
    "ann['T4U_measured'] = ann.apply(lambda row: fillNewAttributes(row,'T4U'), axis=1)\n",
    "ann['FTI_measured'] = ann.apply(lambda row: fillNewAttributes(row,'FTI'), axis=1)\n",
    "display(ann.dtypes)\n",
    "### Now I can merge all the datasets in one:\n",
    "data = pd.concat([allDataset,thyroid0387,hypothyroid,sick_euthyroid,ann], ignore_index = True)\n",
    "display(data.shape)\n",
    "display(data.dtypes)\n",
    "# Part 2: Data pre processing\n",
    "### I will start the data pre processing observing the set of possible values for each attribute:\n",
    "for column in data.columns:\n",
    "    listOfValues=set(data[column])\n",
    "    print(column,\": \",listOfValues)\n",
    "### Sometimes '?' has been used  instead of 'nan', so before counting how many nans are present, I need to do a substitoution:\n",
    "data=data.replace({\"?\":np.NAN})\n",
    "data.isna().sum()\n",
    "### The 'TBG', 'referral_source' and 'TBG_measured' attributes have too many nan values, I have to drop them. Let's try to drop the 'sex' attribute too:\n",
    "del data['TBG']\n",
    "del data['referral_source']\n",
    "del data['TBG_measured']\n",
    "del data['sex']\n",
    "### I can have maximum nine nan values in a row, so I will drop all the rows wtih more than five nan values because they present very few data and aren't good enough for the classification:\n",
    "data.dropna(axis = 0, thresh = 20, inplace = True)\n",
    "data.isna().sum()\n",
    "### For the classification is important that the dataset only has numerical attributes, so I have to encode the categorical values into numerical values:\n",
    "data = data.replace({\"t\":1,\"f\":0, \"y\":1, \"n\":0, \"hypothyroid\":1, \"negative\":0,\"hyperthyroid\":2, \"F\":1, \"M\":0})\n",
    "display(data.dtypes)\n",
    "cols = data.columns[data.dtypes.eq('object')]\n",
    "data[cols] = data[cols].apply(pd.to_numeric, errors='coerce')\n",
    "display(data.dtypes)\n",
    "# Part 3: training of the classifiers\n",
    "### Before the training starts, I have to find the attributes most related to the target:\n",
    "corr_values = abs(data[data.columns[0:]].corr()['Target'][:])\n",
    "corr_values = corr_values.drop('Target')\n",
    "corr_values = corr_values[corr_values > 0.04]\n",
    "display(corr_values)\n",
    "### Another thing that I have to do is to divide the dataset into two sets: the training set and the testing set.\n",
    "def holdout(dataframe):\n",
    "  x = dataframe[corr_values.index]\n",
    "  y = dataframe['Target']\n",
    "  X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.30, random_state=42) \n",
    "  return X_train, X_test, y_train, y_test\n",
    "\n",
    "X_train, X_test, y_train, y_test = holdout(data)\n",
    "### Now I will define the classifiers that I'm going to use. I need some classifiers that are friendly with nan values:\n",
    "classifiers = {\n",
    "    \"XGBClassifier\" : XGBClassifier(learning_rate=0.01),\n",
    "    \"CatBoostClassifier\" : CatBoostClassifier(max_depth=4,verbose=0),\n",
    "}\n",
    "### Now it's time to train the classifiers and discuss the results:\n",
    "def classification(classifiers, X_train, X_test, y_train, y_test):\n",
    "    # Creo un dataframe per visualizzare i risultati calcolati\n",
    "  res = pd.DataFrame(columns=[\"Classifier\", \n",
    "                                \"Accuracy\", \n",
    "                                \"Precision\", \n",
    "                                \"Recall\", \n",
    "                                \"FScore\"])\n",
    "  for name, clf in classifiers.items():\n",
    "            clf.fit(X_train, y_train)\n",
    "            y_pred = clf.predict(X_test)\n",
    "            pr, rc, fs, sup = metrics.precision_recall_fscore_support(y_test, y_pred, average='macro')\n",
    "            res = res.append({\"Classifier\": name,\"Accuracy\": round(metrics.accuracy_score(y_test, y_pred), 4),\n",
    "                              \"Precision\": round(pr, 4), \"Recall\":round(rc, 4), \"FScore\":round(fs, 4)}, ignore_index=True)\n",
    "            print(\"Confusion matrix for: \", name)\n",
    "            display(confusion_matrix(y_test, y_pred))\n",
    "  res.set_index(\"FScore\", inplace=True)\n",
    "  res.sort_values(by=\"FScore\", ascending=False, inplace=True)   \n",
    "  return res\n",
    "\n",
    "display(classification(classifiers, X_train, X_test, y_train, y_test))\n",
    "display(data.shape)\n",
    "data.Target.value_counts()\n",
    "### As we can see, 'data' is rather unbalanced, so the accurancy isn't a very good metric. I have a serious problem with the third class. I should try some alternatives transformation and see how the results change.\n",
    "# Part 4: Alternative transformations\n",
    "### First of all, I will try to fill the nan values with the spline interpolation:\n",
    "data1 = data.interpolate(method = 'spline', order = 3)\n",
    "display(data1.isna().sum())\n",
    "### Now I have a new dataset, so I have to repeat all the steps previous to the evalutation of the results. Also, now that there aren't nan values, it's possible to use more classifiers:\n",
    "classifiers1 = {\n",
    "    \"XGBClassifier\" : XGBClassifier(learning_rate=0.01),\n",
    "    \"CatBoostClassifier\" : CatBoostClassifier(max_depth=4,verbose=0),\n",
    "    \"Nearest Neighbors\" : KNeighborsClassifier(4),\n",
    "    \"Decision Tree\" : DecisionTreeClassifier(class_weight = 'balanced'),\n",
    "    \"Random Forest\": RandomForestClassifier(class_weight = 'balanced',random_state = 1),\n",
    "    \"ExtraTrees\": ExtraTreesClassifier(class_weight = 'balanced',random_state = 1),\n",
    "    \"MLPClassifier\": MLPClassifier(hidden_layer_sizes=(256,128,64,32),activation=\"relu\",random_state=1)\n",
    "}\n",
    "corr_values = abs(data1[data1.columns[0:]].corr()['Target'][:])\n",
    "corr_values = corr_values.drop('Target')\n",
    "corr_values = corr_values[corr_values > 0.04]\n",
    "display(corr_values)\n",
    "\n",
    "X_train1, X_test1, y_train1, y_test1 = holdout(data1)\n",
    "\n",
    "display(classification(classifiers1,X_train1, X_test1, y_train1, y_test1))\n",
    "### So, we can see that the results haven't changed that much. The most related attributes are the same and the FScore improved slightly. Let's check if there are any differences after a discretization:\n",
    "def fdiscretizer(attribute,dataframe):\n",
    "    enc = LabelEncoder()\n",
    "    dataframe[attribute] = pd.qcut(dataframe[attribute], 20, duplicates='drop')\n",
    "    dataframe[attribute] = enc.fit_transform(dataframe[attribute])\n",
    "    dataframe = dataframe.convert_dtypes(convert_integer=True)\n",
    "\n",
    "data2 = data1.copy()\n",
    "fdiscretizer('age',data2)\n",
    "fdiscretizer('TSH',data2)\n",
    "fdiscretizer('T3',data2)\n",
    "fdiscretizer('TT4',data2)\n",
    "fdiscretizer('T4U',data2)\n",
    "fdiscretizer('FTI',data2)\n",
    "corr_values = abs(data2[data2.columns[0:]].corr()['Target'][:])\n",
    "corr_values = corr_values.drop('Target')\n",
    "corr_values = corr_values[corr_values > 0.04]\n",
    "display(corr_values)\n",
    "\n",
    "X_train2, X_test2, y_train2, y_test2 = holdout(data2)\n",
    "\n",
    "display(classification(classifiers1,X_train2, X_test2, y_train2, y_test2))\n",
    "### Now 'T3' is one of the most related attributes, but there aren't major changes in the results. FScore got slightly worse.  Maybe it's possible to apply a normalization istead of a discretization:\n",
    "data3 = ((data1-data1.min())/(data1.max()-data1.min()))*20\n",
    "\n",
    "corr_values = abs(data3[data3.columns[0:]].corr()['Target'][:])\n",
    "corr_values = corr_values.drop('Target')\n",
    "corr_values = corr_values[corr_values > 0.04]\n",
    "display(corr_values)\n",
    "\n",
    "X_train3, X_test3, y_train3, y_test3 = holdout(data3)\n",
    "\n",
    "display(classification(classifiers1,X_train3, X_test3, y_train3, y_test3))\n",
    "### Now 'T3' has gone again and we can see a tiny improvement from the last time. To obtain better results, I should try to balance the dataset by over-sampling (by adding more samples from under-represented classes) or by under-sampling (by removing samples from over-represented classes). Let's start with an over-sampling method:\n",
    "smote = SMOTE('not majority',random_state = 1)\n",
    "X_train_sm, y_train_sm = smote.fit_sample(X_train3,y_train3)\n",
    "X_test_sm, y_test_sm = smote.fit_sample(X_test3,y_test3)\n",
    "display(X_train3.shape)\n",
    "display(X_train_sm.shape)\n",
    "display(classification(classifiers1,X_train_sm, X_test_sm, y_train_sm, y_test_sm))\n",
    "### So, we got a huge improvement on all the metrics' score, except for Accuracy. But now that the data is balanced, it is a valid metric too. Now it's time to try an under-sampling method: \n",
    "df_negative = data3[data3.Target==0]\n",
    "df_hyperthyroid = data3[data3.Target==20]\n",
    "df_hypothyroid = data3[data3.Target==10]\n",
    "\n",
    "df_negative_downsampled = resample(df_negative,replace=False,n_samples=450,random_state=123)\n",
    "df_hypothyroid_downsampled = resample(df_hypothyroid,replace=False,n_samples=450,random_state=123)\n",
    "\n",
    "df_downsampled = pd.concat([df_negative_downsampled,df_hypothyroid_downsampled,df_hyperthyroid])\n",
    "df_downsampled.Target.value_counts()\n",
    "X_train4, X_test4, y_train4, y_test4 = holdout(df_downsampled)\n",
    "display(classification(classifiers1,X_train4, X_test4, y_train4, y_test4))\n",
    "### Using the same attributes, with the under-sampling of the dataset we got very good results, even if there weren't that many rows.\n",
    "# Part 5: Final Discussion\n",
    "### The final dataset that I got is very umbalanced, that's true, but it's normal because only a small percentage of the world population suffers of thyroid disease. Nevertheless, thanks to a good pre-elaboration of the data, I got some very accurate classifiers, that have a good FScore too. I could handle the nan values beacause the results didn't get much worse, and even with the normalization and the discretization they didn't change that much. After having balanced the normalized dataset, we got the best results of the notebook, this means that the work that had been done before was pretty good. In the future, it would be interesting to continue these studies hoping to use more data."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
